{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(lyrics):\n",
    "    if lyrics is None:\n",
    "        return lyrics\n",
    "    \n",
    "    # combine lists of tokens into single string\n",
    "    lyrics = ' '.join(lyrics)\n",
    "            \n",
    "    # remove apostrophes\n",
    "    lyrics = lyrics.replace('\\'', '')\n",
    "            \n",
    "    # remove song structure tags or instructions in brackets\n",
    "    lyrics = re.sub(r'[\\*\\[|\\(|\\{].*\\n*.*[\\]\\)\\}\\*]' , ' ', lyrics)\n",
    "   \n",
    "    # remove variations of Verse 1, VERSE 2, etc...\n",
    "    for verse in ['verse', 'VERSE', 'Verse']:\n",
    "        lyrics = re.sub(verse+' \\d*', '', lyrics)\n",
    "    \n",
    "    # some structure markers formatted as allcaps without brackets\n",
    "    for word in ['OUTRO', 'INSTRUMENTAL', 'PRE', 'HOOK',\n",
    "                 'PRODUCED', 'REFRAIN', 'POST', 'REPEAT', '2x', '3x', '4x',\n",
    "                 'CHORUS', 'INTRO', 'INTERLUDE']:\n",
    "        lyrics = lyrics.replace(word, '')\n",
    "        \n",
    "    # remove varations of Chorus\n",
    "    lyrics = re.sub(r'\\n*Chorus:*.*' , ' ', lyrics)\n",
    "    lyrics = re.sub(r'^Chorus:*.*' , ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nRepeat [C|c]horus:*.*' , ' ', lyrics)\n",
    "    \n",
    "    # remove variations of Intro\n",
    "    lyrics = re.sub(r'Intro[\\s|\\n|:].*', ' ', lyrics)\n",
    "    \n",
    "    # remove variations of Instrumental\n",
    "    lyrics = re.sub(r'-+.*[i|I]nstrumental.*-+', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nBrief instrumental.*\\n', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nInstrumental', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nInstrumental break', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nInstrumental--', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\n~Instrumental~', ' ', lyrics)\n",
    "    \n",
    "    # remove variations of Bridge\n",
    "    lyrics = re.sub(r'\\n\\[*Bridge:\\[*', ' ', lyrics)\n",
    "    \n",
    "    # remove variations of Hook\n",
    "    lyrics = re.sub(r'Hook:.*', ' ', lyrics)\n",
    "    \n",
    "    # remove varations of Repeat\n",
    "    lyrics = re.sub(r'Repeat\\s.*', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nRepeat$', ' ', lyrics)\n",
    "    \n",
    "    # remove credits\n",
    "    lyrics = re.sub(r'.*[P|p]roduced [B|b]y.*', ' ', lyrics)\n",
    "    lyrics = re.sub(r'.*[W|w]ritten [B|b]y.*', ' ', lyrics)\n",
    "    \n",
    "    # remove strays and typos\n",
    "    lyrics = re.sub(r'\\[Outro\\[', ' ', lyrics)\n",
    "    lyrics = re.sub(r'Sax & background & instrumental\\)', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nSource: ', ' ', lyrics)\n",
    "    lyrics = re.sub(r'Shotgun 2: 58 Trk 1 \\n  \\nJr. Walker & The All Stars '\\\n",
    "                    +'\\nAnd/or The Funk Brothers - instrumental \\nPop Chart '\\\n",
    "                    +'#4 Feb 13, 1965 \\nSoul Label - 35008   \\n ', ' ', lyrics)\n",
    "    lyrics = re.sub(r'- musical interlude -', ' ', lyrics)\n",
    "    lyrics = re.sub(r'\\nRefrain:', ' ', lyrics)\n",
    "            \n",
    "    # replace all punctuations with spaces\n",
    "    lyrics = re.sub(r'[^\\w\\s]', ' ', lyrics)\n",
    "            \n",
    "    # replace consecutive whitespaces with single space\n",
    "    lyrics = re.sub(r'\\s+', ' ', lyrics)\n",
    "    \n",
    "    # convert all tokens to lowercase\n",
    "    lyrics = lyrics.lower()\n",
    "\n",
    "    if lyrics[:29] == 'we do not have the lyrics for' or lyrics == 'instrumental':\n",
    "        lyrics = None\n",
    "    return lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/top_hits_lyrics.json') as json_file:\n",
    "    top_hits_lyrics = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1rfofaqEpACxVEHIZBJe6W',\n",
       " 'lyrics': ['[Intro: Pharrell Williams]',\n",
       "  '\\nHey',\n",
       "  '\\n',\n",
       "  '\\n[Chorus: Camila Cabello & ',\n",
       "  'Pharrell Williams',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'Havana, ooh na-na (',\n",
       "  'Ayy',\n",
       "  ')',\n",
       "  '\\nHalf of my heart is in Havana, ooh na-na (',\n",
       "  'Ayy, ayy',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'He took me back to East Atlanta, na-na-na, ah',\n",
       "  '\\n',\n",
       "  'Oh, but my heart is in Havana (',\n",
       "  'Ayy',\n",
       "  ')',\n",
       "  \"\\nThere's somethin' 'bout his manners (\",\n",
       "  'Uh-huh',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'Havana, ooh na-na (',\n",
       "  'Uh',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  '\\n[Verse 1: Camila Cabello & ',\n",
       "  'Pharrell Williams',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'He didn\\'t walk up with that \"how you doin\\'?\" (',\n",
       "  'Uh',\n",
       "  ')',\n",
       "  '\\nWhen he came in the room',\n",
       "  \"\\nHe said there's a lot of girls I can do with (\",\n",
       "  'Uh',\n",
       "  ')',\n",
       "  \"\\nBut I can't without you\",\n",
       "  '\\nI knew him forever in a minute (',\n",
       "  'Hey',\n",
       "  ')',\n",
       "  '\\nThat summer night in June',\n",
       "  '\\n',\n",
       "  'And papa says he got malo in him (',\n",
       "  'Uh',\n",
       "  ')',\n",
       "  \"\\nHe got me feelin' like...\",\n",
       "  '\\n',\n",
       "  '\\n[Pre-Chorus: Camila Cabello & ',\n",
       "  'Pharrell Williams',\n",
       "  ']',\n",
       "  '\\nOoh, ooh-ooh-ooh-ooh-ooh-ooh-ooh (Ayy)',\n",
       "  '\\nI knew it when I met him (Ayy), I loved him when I left him',\n",
       "  \"\\nGot me feelin' like, ooh, ooh-ooh-ooh-ooh-ooh-ooh-ooh\",\n",
       "  '\\nAnd then I had to tell him, I had to go',\n",
       "  '\\nOh na-na-na-na-na (',\n",
       "  'Woo',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  '\\n[Chorus: Camila Cabello & ',\n",
       "  'Pharrell Williams',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'Havana, ooh na-na (',\n",
       "  'Ayy, hey',\n",
       "  ')',\n",
       "  '\\nHalf of my heart is in Havana, ooh na-na (',\n",
       "  'Ayy, ayy, uh-huh',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'He took me back to East Atlanta, na-na-na',\n",
       "  '\\n',\n",
       "  'Oh, but my heart is in Havana (',\n",
       "  'Huh',\n",
       "  ')',\n",
       "  '\\nMy heart is in Havana (',\n",
       "  'Ayy',\n",
       "  ')',\n",
       "  '\\nHavana, ooh na-na',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '[Verse 2: Young Thug]',\n",
       "  '\\n',\n",
       "  '(Jeffery)',\n",
       "  '\\n',\n",
       "  'Just graduated, fresh on campus, mmm',\n",
       "  '\\nFresh out East Atlanta with no manners, damn (Fresh out East Atlanta)',\n",
       "  '\\nBump on her bumper like a traffic jam',\n",
       "  '\\n',\n",
       "  'Hey, I was quick to pay that girl like Uncle Sam (Here you go, ayy)',\n",
       "  '\\n',\n",
       "  'Back it on me (Back it up)',\n",
       "  \"\\nShawty cravin' on me, get to eatin' on me (On me)\",\n",
       "  '\\nShe waited on me (And what?)',\n",
       "  \"\\nShawty cakin' on me, got the bacon on me (Wait up)\",\n",
       "  \"\\nThis is history in the makin', homie (Homie)\",\n",
       "  '\\n',\n",
       "  'Point blank, close range, that B (Tah, tah)',\n",
       "  \"\\nIf it cost a million, that's me (That's me)\",\n",
       "  '\\n',\n",
       "  \"I was gettin' mula, baby\",\n",
       "  '\\n',\n",
       "  '\\n[Chorus: Camila Cabello & ',\n",
       "  'Pharrell ',\n",
       "  'Williams',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'Havana, ooh na-na (',\n",
       "  'Ayy, ayy',\n",
       "  ')',\n",
       "  '\\nHalf of my heart is in Havana, ooh na-na (Oh, ',\n",
       "  'ayy, ayy, uh-huh',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'He took me back to East Atlanta, na-na-na (Oh, no)',\n",
       "  '\\n',\n",
       "  'Oh, but my heart is in Havana (',\n",
       "  'Huh',\n",
       "  ')',\n",
       "  '\\nMy heart is in Havana (',\n",
       "  'Ayy',\n",
       "  ')',\n",
       "  '\\nHavana, ooh na-na',\n",
       "  '\\n',\n",
       "  '\\n[Bridge: Starrah & ',\n",
       "  'Camila Cabello',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'Ooh na-na, oh, na-na-na (',\n",
       "  'Ooh, ooh-ooh-ooh-ooh-ooh-ooh',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'Take me back, back, back like...',\n",
       "  '\\nOoh na-na, oh, na-na-na (Yeah, babe)',\n",
       "  '\\n',\n",
       "  'Take me back, back, back like...',\n",
       "  '\\nOoh na-na, oh, na-na-na (Yeah, yeah)',\n",
       "  '\\n',\n",
       "  'Take me back, back, back like...',\n",
       "  '\\nOoh na-na, oh, na-na-na (Yeah, babe)',\n",
       "  '\\n',\n",
       "  'Take me back, back, back',\n",
       "  '\\n',\n",
       "  'Hey, hey...',\n",
       "  '\\nOoh, ooh-ooh-ooh-ooh-ooh-ooh-ooh (Hey)',\n",
       "  '\\nOoh, ooh-ooh-ooh-ooh-ooh-ooh-ooh (Hey)',\n",
       "  '\\nTake me back to my Havana...',\n",
       "  '\\n',\n",
       "  '\\n[Chorus: Camila Cabello & ',\n",
       "  'Pharrell Williams',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'Havana, ooh na-na',\n",
       "  '\\nHalf of my heart is in Havana, ooh na-na (Oh, yeah)',\n",
       "  '\\n',\n",
       "  'He took me back to East Atlanta, na-na-na (',\n",
       "  'Ayy, ayy',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  'Oh, but my heart is in Havana',\n",
       "  '\\nMy heart is in Havana (',\n",
       "  'Ayy',\n",
       "  ')',\n",
       "  '\\nHavana, ooh na-na (',\n",
       "  'Uh-huh',\n",
       "  ')',\n",
       "  '\\n',\n",
       "  '\\n[Outro: Starrah & ',\n",
       "  'Camila Cabello',\n",
       "  ']',\n",
       "  '\\n',\n",
       "  'Oh, na-na-na (',\n",
       "  'Oh, na, yeah',\n",
       "  ')',\n",
       "  '\\nOh, na-na-na',\n",
       "  '\\nOh, na-na-na (',\n",
       "  'No, no, no, take me back',\n",
       "  ')',\n",
       "  '\\nOh, na-na-na',\n",
       "  '\\n',\n",
       "  'Havana, ooh na-na'],\n",
       " 'source': 'genius.com'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_hits_lyrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hits_df = pd.DataFrame(top_hits_lyrics)\n",
    "top_hits_df['clean_lyrics'] = top_hits_df['lyrics'].apply(lambda x: clean_lyrics(x))\n",
    "top_hits_df = top_hits_df[top_hits_df['source'].notnull()]\n",
    "top_hits_df = top_hits_df[top_hits_df['clean_lyrics'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2805, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_hits_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/songs_lyrics_5000.json') as json_file:\n",
    "    not_hits_1 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/songs_lyrics_10000.json') as json_file:\n",
    "    not_hits_2 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hits_lyrics = not_hits_1 + not_hits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/not_hits_lyrics.json', 'w') as f:\n",
    "        json.dump(not_hits_lyrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hits_df = pd.DataFrame(not_hits_lyrics)\n",
    "not_hits_df['clean_lyrics'] = not_hits_df['lyrics'].apply(lambda x: clean_lyrics(x))\n",
    "not_hits_df = not_hits_df[not_hits_df['source'].notnull()]\n",
    "not_hits_df = not_hits_df[not_hits_df['clean_lyrics'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7937, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_hits_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: stratified sampling by decade\n",
    "not_hits_df = not_hits_df.sample(n=top_hits_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2805, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_hits_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk out to winter swear ill be there chill will wake you high and dry youll wonder why we met in the summer and walked til the fall and breathless we talked it was tongues despite what theyll say it wasnt youth wed hit the truth faces of strummer that fell from your wall and nothing was left where they hung so sweet and bitter theyre what we found so drink them down and walk out to winter swear ill be there chill will wake you high and dry youll wonder why walk out to winter swear ill be there chance is buried just below the blinding snow you burn in the breadline and ribbons and all so walk to winter you wont be late youll always wait this generation the walk to the wall but im not angry get your gear get out of here and walk out to winter swear ill be there chill will wake you high and dry youll wonder why walk out to winter swear ill be there chance is buried just below the blinding snow walk out to winter swear ill be there chill will wake you high and dry walk out to winter swear ill be there youll find snowblind this is life this is life\n"
     ]
    }
   ],
   "source": [
    "print(not_hits_df['clean_lyrics'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hits_df['class'] = 1\n",
    "not_hits_df['class'] = 0\n",
    "df = pd.concat([top_hits_df, not_hits_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5610, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(lemmatizer.lemmatize(word.lower()))\n",
    "    return tokens\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_lyrics']), tags=[r['class']]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_lyrics']), tags=[r['class']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['it', 'wa', 'the', 'third', 'of', 'september', 'that', 'day', 'ill', 'always', 'remember', 'yes', 'will', 'cause', 'that', 'wa', 'the', 'day', 'that', 'my', 'daddy', 'died', 'never', 'got', 'chance', 'to', 'see', 'him', 'never', 'heard', 'nothin', 'but', 'bad', 'thing', 'about', 'him', 'momma', 'im', 'depending', 'on', 'you', 'to', 'tell', 'me', 'the', 'truth', 'momma', 'just', 'hung', 'her', 'head', 'and', 'said', 'son', 'well', 'well', 'hey', 'momma', 'is', 'it', 'true', 'what', 'they', 'say', 'that', 'papa', 'never', 'worked', 'day', 'in', 'his', 'life', 'and', 'momma', 'some', 'bad', 'talk', 'goin', 'round', 'town', 'sayin', 'that', 'papa', 'had', 'three', 'outside', 'child', 'and', 'another', 'wife', 'and', 'that', 'aint', 'right', 'heard', 'them', 'talking', 'papa', 'doing', 'some', 'store', 'front', 'preachin', 'talking', 'about', 'saving', 'soul', 'and', 'all', 'the', 'time', 'leechin', 'dealing', 'in', 'dirt', 'and', 'stealing', 'in', 'the', 'name', 'of', 'the', 'lord', 'momma', 'just', 'hung', 'her', 'head', 'and', 'said', 'my', 'son', 'hey', 'uhh', 'hey', 'momma', 'heard', 'papa', 'called', 'himself', 'jack', 'of', 'all', 'trade', 'tell', 'me', 'is', 'that', 'what', 'sent', 'papa', 'to', 'an', 'early', 'grave', 'folk', 'say', 'papa', 'would', 'beg', 'borrow', 'steal', 'to', 'pay', 'his', 'bill', 'hey', 'momma', 'folk', 'say', 'papa', 'never', 'wa', 'much', 'on', 'thinking', 'spent', 'most', 'of', 'his', 'time', 'chasing', 'woman', 'and', 'drinking', 'momma', 'im', 'depending', 'on', 'you', 'to', 'tell', 'me', 'the', 'truth', 'momma', 'looked', 'up', 'with', 'tear', 'in', 'her', 'eye', 'and', 'said', 'son', 'well', 'well', 'well', 'well', 'all', 'he', 'left', 'u', 'wa', 'alone', 'lone', 'lone', 'lone', 'alone', 'hey', 'yeah', 'all', 'he', 'left', 'u', 'wa', 'alone', 'say', 'yeah', 'yes', 'he', 'wa', 'all', 'he', 'left', 'u', 'wa', 'alone', 'my', 'daddy', 'wa', 'ahhhh', 'ha', 'ha'], tags=[0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3927/3927 [00:00<00:00, 1307017.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3927/3927 [00:00<00:00, 1911679.64it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1380755.45it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1307328.50it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2599594.67it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2732874.03it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2075221.34it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2637897.47it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2867519.47it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2330037.04it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1416375.60it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2488823.18it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2505099.89it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2909562.23it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2468309.88it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1436510.71it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2511210.83it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2448495.88it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1839311.20it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2507006.36it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2340965.29it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2260330.97it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2532835.89it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2441237.86it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2391959.31it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1450936.56it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 1452728.15it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2058107.19it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2499777.18it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2549300.70it/s]\n",
      "100%|██████████| 3927/3927 [00:00<00:00, 2423636.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.5 s, sys: 608 ms, total: 58.1 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkrsteska/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.471182412358883\n",
      "Testing F1 score: 0.4694016890822741\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_lyrics'] # the features we want to analyze\n",
    "ylabels = df['class'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Accuracy: 0.5971479500891266\n",
      " Precision: [0.57351291 0.62373737]\n",
      " Recall: [0.631644   0.56521739]\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\" test Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\" Precision:\",metrics.precision_score(y_test, predicted, average=None))\n",
    "print(\" Recall:\",metrics.recall_score(y_test, predicted, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Accuracy: 0.5977421271538919\n",
      " Precision: [0.58847185 0.60512273]\n",
      " Recall: [0.54264524 0.64874142]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\" test Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\" Precision:\",metrics.precision_score(y_test, predicted, average=None))\n",
    "print(\" Recall:\",metrics.recall_score(y_test, predicted, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/top_hits.json') as json_file:\n",
    "    top_hits = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hits_songs_df = pd.DataFrame(top_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hits_merged_df = pd.merge(top_hits_df, top_hits_songs_df, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2805, 37)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_hits_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/songs.json') as json_file:\n",
    "    not_hits = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hits_songs_df = pd.DataFrame(not_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hits_merged_df = pd.merge(not_hits_df, not_hits_songs_df, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2805, 37)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_hits_merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([top_hits_merged_df, not_hits_merged_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = ['acousticness', 'danceability',  'energy',\n",
    "            'instrumentalness', 'liveness', 'loudness', 'mode',\n",
    "            'speechiness', 'tempo', 'time_signature', 'valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[audio_features]\n",
    "y = merged_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Accuracy: 0.6488413547237076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkrsteska/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(\" test Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.184</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>-4.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>104.988</td>\n",
       "      <td>4</td>\n",
       "      <td>0.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.228</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0967</td>\n",
       "      <td>-4.353</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>178.086</td>\n",
       "      <td>4</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>-7.226</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>98.963</td>\n",
       "      <td>4</td>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>-7.365</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>94.992</td>\n",
       "      <td>3</td>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>-6.458</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0856</td>\n",
       "      <td>154.084</td>\n",
       "      <td>4</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  energy  instrumentalness  liveness  loudness  \\\n",
       "0         0.184         0.765   0.523          0.000036    0.1320    -4.333   \n",
       "1         0.228         0.653   0.816          0.000000    0.0967    -4.353   \n",
       "2         0.463         0.731   0.469          0.000001    0.1030    -7.226   \n",
       "3         0.779         0.587   0.299          0.000000    0.1230    -7.365   \n",
       "4         0.047         0.643   0.783          0.000000    0.0830    -6.458   \n",
       "\n",
       "   mode  speechiness    tempo  time_signature  valence  \n",
       "0     1       0.0300  104.988               4    0.394  \n",
       "1     1       0.1670  178.086               4    0.816  \n",
       "2     1       0.0326   98.963               4    0.631  \n",
       "3     1       0.0263   94.992               3    0.356  \n",
       "4     1       0.0856  154.084               4    0.579  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
