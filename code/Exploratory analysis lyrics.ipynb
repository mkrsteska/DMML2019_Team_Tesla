{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import multiprocessing\n",
        "from gensim.models import Doc2Vec\n",
        "from tqdm import tqdm\n",
        "from sklearn import utils\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_lyrics(lyrics):\n",
        "    if lyrics is None:\n",
        "        return lyrics\n",
        "    \n",
        "    # combine lists of tokens into single string\n",
        "    lyrics = ' '.join(lyrics)\n",
        "            \n",
        "    # remove apostrophes\n",
        "    lyrics = lyrics.replace('\\'', '')\n",
        "            \n",
        "    # remove song structure tags or instructions in brackets\n",
        "    lyrics = re.sub(r'[\\*\\[|\\(|\\{].*\\n*.*[\\]\\)\\}\\*]' , ' ', lyrics)\n",
        "   \n",
        "    # remove variations of Verse 1, VERSE 2, etc...\n",
        "    for verse in ['verse', 'VERSE', 'Verse']:\n",
        "        lyrics = re.sub(verse+' \\d*', '', lyrics)\n",
        "    \n",
        "    # some structure markers formatted as allcaps without brackets\n",
        "    for word in ['OUTRO', 'INSTRUMENTAL', 'PRE', 'HOOK',\n",
        "                 'PRODUCED', 'REFRAIN', 'POST', 'REPEAT', '2x', '3x', '4x',\n",
        "                 'CHORUS', 'INTRO', 'INTERLUDE']:\n",
        "        lyrics = lyrics.replace(word, '')\n",
        "        \n",
        "    # remove varations of Chorus\n",
        "    lyrics = re.sub(r'\\n*Chorus:*.*' , ' ', lyrics)\n",
        "    lyrics = re.sub(r'^Chorus:*.*' , ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nRepeat [C|c]horus:*.*' , ' ', lyrics)\n",
        "    \n",
        "    # remove variations of Intro\n",
        "    lyrics = re.sub(r'Intro[\\s|\\n|:].*', ' ', lyrics)\n",
        "    \n",
        "    # remove variations of Instrumental\n",
        "    lyrics = re.sub(r'-+.*[i|I]nstrumental.*-+', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nBrief instrumental.*\\n', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nInstrumental', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nInstrumental break', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nInstrumental--', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\n~Instrumental~', ' ', lyrics)\n",
        "    \n",
        "    # remove variations of Bridge\n",
        "    lyrics = re.sub(r'\\n\\[*Bridge:\\[*', ' ', lyrics)\n",
        "    \n",
        "    # remove variations of Hook\n",
        "    lyrics = re.sub(r'Hook:.*', ' ', lyrics)\n",
        "    \n",
        "    # remove varations of Repeat\n",
        "    lyrics = re.sub(r'Repeat\\s.*', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nRepeat$', ' ', lyrics)\n",
        "    \n",
        "    # remove credits\n",
        "    lyrics = re.sub(r'.*[P|p]roduced [B|b]y.*', ' ', lyrics)\n",
        "    lyrics = re.sub(r'.*[W|w]ritten [B|b]y.*', ' ', lyrics)\n",
        "    \n",
        "    # remove strays and typos\n",
        "    lyrics = re.sub(r'\\[Outro\\[', ' ', lyrics)\n",
        "    lyrics = re.sub(r'Sax & background & instrumental\\)', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nSource: ', ' ', lyrics)\n",
        "    lyrics = re.sub(r'Shotgun 2: 58 Trk 1 \\n  \\nJr. Walker & The All Stars '\\\n",
        "                    +'\\nAnd/or The Funk Brothers - instrumental \\nPop Chart '\\\n",
        "                    +'#4 Feb 13, 1965 \\nSoul Label - 35008   \\n ', ' ', lyrics)\n",
        "    lyrics = re.sub(r'- musical interlude -', ' ', lyrics)\n",
        "    lyrics = re.sub(r'\\nRefrain:', ' ', lyrics)\n",
        "            \n",
        "    # replace all punctuations with spaces\n",
        "    lyrics = re.sub(r'[^\\w\\s]', ' ', lyrics)\n",
        "            \n",
        "    # replace consecutive whitespaces with single space\n",
        "    lyrics = re.sub(r'\\s+', ' ', lyrics)\n",
        "    \n",
        "    # convert all tokens to lowercase\n",
        "    lyrics = lyrics.lower()\n",
        "\n",
        "    if lyrics[:29] == 'we do not have the lyrics for' or lyrics == 'instrumental':\n",
        "        lyrics = None\n",
        "    return lyrics\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../data/top_hits_lyrics.json') as json_file:\n",
        "    top_hits_lyrics = json.load(json_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_lyrics[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_df = pd.DataFrame(top_hits_lyrics)\n",
        "top_hits_df['clean_lyrics'] = top_hits_df['lyrics'].apply(lambda x: clean_lyrics(x))\n",
        "top_hits_df = top_hits_df[top_hits_df['source'].notnull()]\n",
        "top_hits_df = top_hits_df[top_hits_df['clean_lyrics'].notnull()]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_df.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../data/songs_lyrics_5000.json') as json_file:\n",
        "    not_hits_1 = json.load(json_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../data/songs_lyrics_10000.json') as json_file:\n",
        "    not_hits_2 = json.load(json_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_lyrics = not_hits_1 + not_hits_2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../data/not_hits_lyrics.json', 'w') as f:\n",
        "        json.dump(not_hits_lyrics, f)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_df = pd.DataFrame(not_hits_lyrics)\n",
        "not_hits_df['clean_lyrics'] = not_hits_df['lyrics'].apply(lambda x: clean_lyrics(x))\n",
        "not_hits_df = not_hits_df[not_hits_df['source'].notnull()]\n",
        "not_hits_df = not_hits_df[not_hits_df['clean_lyrics'].notnull()]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_df.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: stratified sampling by decade\n",
        "not_hits_df = not_hits_df.sample(n=top_hits_df.shape[0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_df.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(not_hits_df['clean_lyrics'].iloc[0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_df['class'] = 1\n",
        "not_hits_df['class'] = 0\n",
        "df = pd.concat([top_hits_df, not_hits_df])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc2Vec"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(lemmatizer.lemmatize(word.lower()))\n",
        "    return tokens\n",
        "\n",
        "train_tagged = train.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['clean_lyrics']), tags=[r['class']]), axis=1)\n",
        "test_tagged = test.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['clean_lyrics']), tags=[r['class']]), axis=1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_tagged.values[1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cores = multiprocessing.cpu_count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag Of Words"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['clean_lyrics'] # the features we want to analyze\n",
        "ylabels = df['class'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, random_state=72)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression(solver=\"lbfgs\")\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\" test Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "print(\" Precision:\",metrics.precision_score(y_test, predicted, average=None))\n",
        "print(\" Recall:\",metrics.recall_score(y_test, predicted, average=None))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = RandomForestClassifier(n_estimators=1000)\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\" test Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "print(\" Precision:\",metrics.precision_score(y_test, predicted, average=None))\n",
        "print(\" Recall:\",metrics.recall_score(y_test, predicted, average=None))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Features"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../data/top_hits.json') as json_file:\n",
        "    top_hits = json.load(json_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_songs_df = pd.DataFrame(top_hits)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_merged_df = pd.merge(top_hits_df, top_hits_songs_df, on='id', how='inner')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "top_hits_merged_df.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../data/songs.json') as json_file:\n",
        "    not_hits = json.load(json_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_songs_df = pd.DataFrame(not_hits)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_merged_df = pd.merge(not_hits_df, not_hits_songs_df, on='id', how='inner')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "not_hits_merged_df.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat([top_hits_merged_df, not_hits_merged_df])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features = ['acousticness', 'danceability',  'energy',\n",
        "            'instrumentalness', 'liveness', 'loudness', 'mode',\n",
        "            'speechiness', 'tempo', 'time_signature', 'valence']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X = merged_df[audio_features]\n",
        "y = merged_df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, random_state=72)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "predicted = classifier.predict(X_test)\n",
        "print(\" test Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}